{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Deep learning and Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "                        #### 好像很厲害 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 陳均豪 \n",
    "<figure><img src=\"deep_image/funny/253170_10151355741724688_776288579_n.jpg\" alt=\"image\" width=\"300\" height=\"300\" align=\"right\"></figure>\n",
    "\n",
    "- 物理系不務正業\n",
    "- 不小心~~誤入歧途~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "0. Machine Learning\n",
    "1. Deep learning \n",
    "2. Theano and techniques\n",
    "3. Application\n",
    "4. Summary\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 今天目標\n",
    "\n",
    "- 概念性介紹 deep learning，數學的部分比較少\n",
    "- 有基本認識後，可以開始進入 deep learning 的領域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 不負責任的亂 demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Machine Learning\n",
    "\n",
    "簡單的介紹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is learning ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/learning.png)\n",
    "\n",
    "Source: <a href=https://www.coursera.org/course/ntumlone>機器學習，林軒田</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 生活中的學習是什麼呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 先來看看小孩子是怎麼學習的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ***觀察***周圍的人的行爲模式，從***錯誤*** 中 學習   ***語言，知識，辨認物體***\n",
    "- 被罵、被打、受到傷害，才學會怎麼保護自己"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 至少要 ~~被咬過~~ 才知道巨人的可怕\n",
    "\n",
    "<figure><img src=\"deep_image/funny/shingeki1-5.jpg\" alt=\"image\" width=\"300\" height=\"300\"></figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is Machine Learning ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 對照與比較，以 supervised learning 爲主\n",
    "\n",
    "| 比較 |人類 | 機器 |\n",
    "|:--- |:--- |:---- |\n",
    "|觀察 | 小孩子的觀察 | 有意義的 data |\n",
    "|參考答案  | 大人的監督 | 已經有答案的 標籤 |\n",
    "|怎麼學習 | 透過錯誤的嘗試，經驗的累積，不想要再犯錯 | 定義 cost function，使這個 model 的錯誤越小越好 |\n",
    "|學習方式 | 人類腦袋 | 數學模型 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| 比較 |人類 | 機器 |\n",
    "|:--- |:--- |:---- |\n",
    "|錯誤函數| 他X的，我就是要生存啦，怎樣！ | 請給我最低谷，謝謝！ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 來一點資料的 fu\n",
    "![](images/error.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning setup\n",
    "![](images/learning_setup.png)\n",
    "\n",
    "Source: <a href=https://www.coursera.org/course/ntumlone>機器學習，林軒田</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 所以 unsupervised learning 就是，沒有參考答案，而自己學會一些事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 接下來會簡介可能會用到的知識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cost function\n",
    "\n",
    "常用的 cost function: 都是爲了衡量錯誤\n",
    "- L1 Norm:  $Cost = \\frac{1}{N}\\sum_n | f(x_n)-y_n| $\n",
    "- L2 Norm:  $Cost = \\frac{1}{N}\\sum_n( f(x_n)-y_n)^2 $\n",
    "- Cross Entropy: $Cost = -\\frac{1}{N} \\sum_n \\left[y_n \\ln f(x_n) + (1-y_n ) \\ln (1-f(x_n)) \\right]$\n",
    "\n",
    "References: \n",
    "- <a href=http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/>Differences between L1 and L2 as Loss Function and Regularization</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 假設我們的 cost 是 2-Dim\n",
    "\n",
    "![](images/logistic_E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 這裏介紹要如何去逼近最低谷的方法\n",
    "\n",
    "Gradient Descent method (Batch learning):\n",
    "- 每次一小步的前進，叫做 learning rate $\\eta$，慢慢去逼近最低谷\n",
    "- 每次往比較陡的地方前進\n",
    "\n",
    "當然還有 Stochastic Gradient Descent (online learning) - 使用 mini batch 的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](images/logistic_yta.png)\n",
    "\n",
    "Source: <a href=https://www.coursera.org/course/ntumlone>機器學習，林軒田</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 我們沒有 cover 的東西有很多：\n",
    "\n",
    "1. overfitting\n",
    "    - Regularization\n",
    "    - Validation\n",
    "- Model\n",
    "    - PLA\n",
    "    - Linear/ logistic regression\n",
    "    - SVM\n",
    "    - Random forest\n",
    "    - etc...\n",
    "- Transformation\n",
    "- Etc...\n",
    "\n",
    "請參考： <a href=https://www.coursera.org/course/ntumlone>機器學習，林軒田</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](/deep_image/funny/aOd46XW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Deep Learning\n",
    "\n",
    "*** 重大聲明， 有些內容我也還沒有弄明白其中的原理 ***\n",
    "\n",
    "Reference:\n",
    "\n",
    "- 強烈建議看這一篇 http://www.slideshare.net/yandex/yann-le-cun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# History of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "- Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. \n",
    "\n",
    "![](deep_image/tikz0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Input: $x_1, x_2, x_3$ \n",
    "- 如果我們在輸入的時候分別對他們都加上一個權重 $w_1, w_2, w_3$\n",
    "- 那我們就可以根據權重，加總起來後，如果有超過某個 threshold 就是 0， 不然就是 1 做爲結果判斷\n",
    "\n",
    "$ \\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Basic Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Breakthrough\n",
    "\n",
    "- Unsupervised learning\n",
    "- GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Auto encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Compare with other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advanced techniques\n",
    "\n",
    "- Activation function\n",
    "- Cost function\n",
    "- Drop out\n",
    "- Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Activation and Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function width=1000 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "HTML('<iframe src=http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function width=1000 height=350></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Activation function\n",
    "\n",
    "- Softmax\n",
    "- Tanh\n",
    "- Relu\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Softmax\n",
    "\n",
    "- output layer of neural networks， 本來只有一個node 的輸出，變成有很多的 neuron 的輸出\n",
    "- $ a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} $, 第L層 第j個 neuron 的輸出\n",
    "- 特性：\n",
    "    - 確保sum 起來等於一\n",
    "    - 值由 0 到 一\n",
    "    - 那我們就可以有一個 機率分佈 的預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Activation and Cost\n",
    "\n",
    "***For learning speed***\n",
    "- softmax and the log-likelihood cost\n",
    "- sigmoid and the cross-entropy cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Early Stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 我們沒有 cover 的東西有：\n",
    "\n",
    "1. 數學\n",
    "- 更多的 gradient 方法\n",
    "- Etc...\n",
    "\n",
    "請參考： \n",
    "1. <a href=http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function> Online book: Neural Networks and Deep Learning </a> \n",
    "- <a href=http://ufldl.stanford.edu/tutorial/> Online tutorial: Stanford Deep Learning </a>\n",
    "- <a href=http://www.slideshare.net/yandex/yann-le-cun> Learning Hierarchies Of Invariant Features </a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 希望 ~~你們~~ 我 到現在還沒有放棄\n",
    "\n",
    "![](/deep_image/funny/pics_jiyiji1984_1371884281.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Review\n",
    "\n",
    "All is about Structure and Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Road Map of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 接下來就是你的事了\n",
    "\n",
    "![](/deep_image/funny/1300507191Od8gkE1Y_s.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The end，Question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Theano and Techniques\n",
    "\n",
    "![](/deep_image/funny/a3d3A08_460sa_v1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# WHAT’S THEANO?\n",
    "\n",
    "Theano is many things\n",
    "\n",
    "- Programming Language\n",
    "- Linear Algebra Compiler\n",
    "- Python library\n",
    "    - Define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays\n",
    "\n",
    "***NOT ML toolkit but mathematical toolkit***\n",
    "\n",
    "Reference:\n",
    "- <a href=http://deeplearning.net/software/theano/tutorial/index.html#tutorial>Theano tutorial</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Theano features\n",
    "\n",
    "- integration with NumPy\n",
    "- use of GPU\n",
    "- Efficient symbolic differentiation\n",
    "- Speed and stability optimizations\n",
    "- Dynamic C code generation (See compilation flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overview of Theano\n",
    "\n",
    "Using Theano:\n",
    "- Symbolically define mathematical functions\n",
    "    - Automatically derive gradient expressions\n",
    "- Compile expressions into executable functions\n",
    "    - ```python \n",
    "    theano.function ([input params], output) \n",
    "    ```\n",
    "- Execute expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Building Symbolic Expressions\n",
    "\n",
    "Tensor\n",
    "- Scalars\n",
    "- Vectors\n",
    "- Matrices\n",
    "- Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tensor\n",
    "\n",
    "Tensor: multi-dimensional array\n",
    "\n",
    "Order of tensor: dimensionality\n",
    "- 0 th order tensor = scalar\n",
    "- 1 th order tensor = vector\n",
    "- 2 th order tensor = matrix\n",
    "- ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Scalar math\n",
    "\n",
    "```python\n",
    "from theano import tensor as T\n",
    "# Note that theano is fully typed\n",
    "x = T.scalar()\n",
    "y = T.scalar()\n",
    "z = x + y\n",
    "w = z * x\n",
    "a = T.sqrt(w)\n",
    "b = T.exp(a)\n",
    "c = a ** b\n",
    "d = T.log(c)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vector Math\n",
    "\n",
    "```python\n",
    "\n",
    "from theano import tensor as T\n",
    "x = T.vector()\n",
    "y = T.vector()\n",
    "# Scalar math applied elementwise\n",
    "a = x * y\n",
    "# vector dot product\n",
    "b = T.dot(x, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Matrix Math\n",
    "```python\n",
    "\n",
    "from theano import tensor as T\n",
    "x = T.matrix()\n",
    "y = T.matrix()\n",
    "a = T.vector()\n",
    "# Matrix-matrix product\n",
    "b = T.dot(x, y)\n",
    "# Matrix-vector product\n",
    "c = T.dot(x, a)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tensors\n",
    "\n",
    "- Dimensionality defined by length of “broadcastable” argument\n",
    "- Can add (or do other elemwise op) on two tensors with same dimensionality\n",
    "- Duplicate tensors along broadcastable axes to make size match\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from theano import tensor as T\n",
    "tensor3 = T.Tensortype(broadcastable=(False,False, False), dtype=‘float32’)\n",
    "x = tensor3()\n",
    "```\n",
    "\n",
    "Reference:\n",
    "<a href=http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.TensorType.broadcastable>Theano, broadcastable explain</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# theano.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "x = T.scalar()\n",
    "y = T.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from theano import function\n",
    "# first arg is list of SYMBOLIC inputs\n",
    "# second arg is SYMBOLIC output\n",
    "f = function([x, y], x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.0, dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call it with NUMERICAL values\n",
    "# Get a NUMERICAL output\n",
    "f(1., 2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Shared variables\n",
    "\n",
    "- GPU usage\n",
    "- think as a global variable\n",
    "- Modify outside function with get_value() and set_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from theano import shared\n",
    "x = shared(0.)\n",
    "updates = [ (x, x + 1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = function([], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f() # updates\n",
    "x.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(101.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.set_value(100.)\n",
    "f() # updates\n",
    "x.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Manipulating Symbolic Expressions\n",
    "\n",
    "Automatic differentiation\n",
    "- tensor.grad(func, [params])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute gradient : $ \\frac{d(x^2)}{dx} = 2x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x) # gradient\n",
    "f = function([x], gy) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(188.4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(94.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Before Implement MLP in Theano\n",
    "\n",
    "![](/deep_image/funny/aMbQ8OR_460sa_v1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Simple Version\n",
    "\n",
    "```python\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "N = 400 # number of samples\n",
    "feats = 784 # dimensionality of features\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2)) # D generate data, format: (x,y)\n",
    "\n",
    "training_steps = 10000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "```python\n",
    "# declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w_1 = theano.shared(rng.randn(784,300))\n",
    "b_1 = theano.shared(numpy.zeros((300,)))\n",
    "w_2 = theano.shared(rng.randn(300))\n",
    "b_2 = theano.shared(0.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# Construct Theano expression graph\n",
    "p_1 = T.sigmoid(-T.dot(T.sigmoid(-T.dot(x, w_1)-b_1), w_2)-b_2)\n",
    "\n",
    "# probability that target = 1\n",
    "prediction = p_1 > 0.5 # the prediction threshold\n",
    "\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1)     # cross-entropy loss func\n",
    "cost = xent.mean() + 0.01 * (w**2).sum()      # the cost to minimize\n",
    "gw_1, gb_1, gw_2, gb_2 = T.grad(cost, [w_1, b_1, w_2, b_2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# Compile\n",
    "train = theano.function(\n",
    "            inputs = [x, y],\n",
    "            outputs = [prediction, xent],\n",
    "            updates = {w_1 : w_1-0.1*gw_1, b_1 : b_1-0.1*gb_1,\n",
    "                       w_2 : w_2-0.1*gw_2, b_2 : b_2-0.1*gb_2})\n",
    "predict = theano.function(inputs = [x], outputs = prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "    \n",
    "print \"target values for D: \", D[1]\n",
    "print \"predictions on D: \", predict(D[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Complex Version\n",
    "\n",
    "```python\n",
    "srng = RandomStreams(seed=2**30)\n",
    "\n",
    "# translate data to theano data type\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "# initialize weight by random\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def tanh(X):\n",
    "    return (1 + T.tanh(X / 2)) / 2\n",
    "\n",
    "# rectified linear unit\n",
    "def relu(X):\n",
    "    # return T.maximum(X, 0.)\n",
    "    return (X + abs(X)) / 2.\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1.0/(1.0+ T.exp(-X))\n",
    "\n",
    "# softmax\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "    \n",
    "# momentum method\n",
    "def momentum(loss, all_params, update_momentum, update_learning_rate):\n",
    "    all_grads = theano.grad(loss, all_params)\n",
    "    updates = []\n",
    "\n",
    "    for param_i, grad_i in zip(all_params, all_grads):\n",
    "        mparam_i = theano.shared(np.zeros(param_i.get_value().shape,\n",
    "                                          dtype=theano.config.floatX),\n",
    "                                 broadcastable=param_i.broadcastable)\n",
    "        v = update_momentum * mparam_i - update_learning_rate * grad_i\n",
    "        updates.append((mparam_i, v))\n",
    "        updates.append((param_i, param_i + v))\n",
    "\n",
    "    return updates\n",
    "\n",
    "def dropout( layer, p=0.):\n",
    "# http://www.quora.com/How-would-you-implement-drop-out-in-a-deep-neural-network\n",
    "    if p > 0. and p < 1.:\n",
    "        mask = srng.binomial(n=1, p=1-p, size=layer.shape)\n",
    "        output = layer * T.cast(mask, theano.config.floatX)\n",
    "        return output\n",
    "    else:\n",
    "        return layer\n",
    "\n",
    "def model(X, para, p_drop_input, p_drop_hidden, relu_bool, train):\n",
    "    ## assume 6 hidden  layers only\n",
    "    for i in range (len(para)/2):\n",
    "        \n",
    "        if i is 0:\n",
    "            ######## Input layer\n",
    "            if train:\n",
    "                X = dropout(X, p_drop_input)\n",
    "            else:\n",
    "                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n",
    "                \n",
    "            if relu_bool:\n",
    "                h = relu(T.dot(X, para[i*2]) +para[i+1])\n",
    "            else:\n",
    "                h = sigmoid(T.dot(X, para[i*2]) + para[i+1])\n",
    "\n",
    "        elif i is 1:\n",
    "            ######## layer 1\n",
    "            if train:\n",
    "                h = dropout(h, p_drop_hidden[i-1])\n",
    "            else:\n",
    "                # h *= (1-p_drop_hidden[i-1])\n",
    "                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n",
    "            \n",
    "            if (i*2+2) is len(para): ## weight at least have 4, w_h_1, b_1, w_o, b_o\n",
    "                py_xx = softmax(T.dot(h, para[i*2])+ para[i*2+1])\n",
    "                break\n",
    "            \n",
    "            if relu_bool:\n",
    "                h2 = relu(T.dot(h, para[i*2] )+ para[i*2+1])\n",
    "            else: \n",
    "                h2 = sigmoid(T.dot(h, para[i*2] ) +para[i*2+1])\n",
    "        \n",
    "        elif i is 2:\n",
    "            ######## layer 2\n",
    "            if train:\n",
    "                h2 = dropout(h2, p_drop_hidden[i-1])\n",
    "            else:\n",
    "                # h2 *= (1-p_drop_hidden[i-1])\n",
    "                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n",
    "            \n",
    "            if (i*2+2) is len(para): ## weight at least have 6, w_h_1, b_1, w_o, b_o\n",
    "                py_xx = softmax(T.dot(h2, para[i*2])+ para[i*2+1])\n",
    "                break\n",
    "            \n",
    "            if relu_bool:\n",
    "                h3 = relu(T.dot(h2, para[i*2] )+ para[i*2+1])\n",
    "            else:\n",
    "                h3 = sigmoid(T.dot(h2, para[i*2] ) +para[i*2+1])\n",
    "                \n",
    "            \n",
    "        elif i is 3:\n",
    "            ######## layer 3\n",
    "            if train:\n",
    "                h3 = dropout(h3, p_drop_hidden[i-1])\n",
    "            else:\n",
    "                # h3 *= (1-p_drop_hidden[i-1])\n",
    "                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n",
    "            \n",
    "            if (i*2+2) is len(para): ## weight at least have 8, w_h_1, b_1, w_o, b_o\n",
    "                py_xx = softmax(T.dot(h3, para[i*2])+ para[i*2+1])\n",
    "                break\n",
    "            \n",
    "            if relu_bool:\n",
    "                h4 = relu(T.dot(h3, para[i*2] )+ para[i*2+1])\n",
    "            else:\n",
    "                h4 = sigmoid(T.dot(h3, para[i*2] ) +para[i*2+1])\n",
    "                \n",
    "                \n",
    "        elif i is 4:\n",
    "            ######## layer 4\n",
    "            if train:\n",
    "                h4 = dropout(h4, p_drop_hidden[i-1])\n",
    "            else:\n",
    "                # h4 *= (1-p_drop_hidden[i-1])\n",
    "                para[i*2].set_value(para[i*2].get_value()*(1-p_drop_hidden[i-1]))\n",
    "            \n",
    "            if (i*2+2) is len(para): ## weight at least have 10, w_h_1, b_1, w_o, b_o\n",
    "                py_xx = softmax(T.dot(h4, para[i*2])+ para[i*2+1])\n",
    "                break\n",
    "            \n",
    "            if relu_bool:\n",
    "                h5 = relu(T.dot(h4, para[i*2] )+ para[i*2+1])\n",
    "            else:\n",
    "                h5 = sigmoid(T.dot(h4, para[i*2] ) +para[i*2+1])\n",
    "                \n",
    "    return py_xx\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "```python\n",
    "class DNN():    \n",
    "    \n",
    "    def __init__(self, input_shape, activation, hidden_layer,\n",
    "                 batch, max_epochs, eval_size, output_num_units,\n",
    "                 drop_input, drop_hidden,\n",
    "                 patience, up_learning_rate, up_momentum, max_norm):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_layer = hidden_layer\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = True\n",
    "        else:\n",
    "            self.activation = False\n",
    "        \n",
    "        self.batch = batch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eval_size = eval_size\n",
    "        self.output_num_units = output_num_units\n",
    "        \n",
    "        self.up_learning_rate = up_learning_rate\n",
    "        self.up_momentum = up_momentum\n",
    "        \n",
    "        self.drop_input = drop_input\n",
    "        self.drop_hidden = drop_hidden\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.best_valid = -np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_params = None\n",
    "        \n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "        self.train_history_ = []\n",
    "\n",
    "        self.params = []\n",
    "        for i in range(len(hidden_layer)):\n",
    "            if i is 0:\n",
    "                w_h_1 = init_weights((self.input_shape[1], self.hidden_layer[i]))\n",
    "                b_1 = build_shared_zeros(self.hidden_layer[i])\n",
    "                self.params.append(w_h_1)\n",
    "                self.params.append(b_1)\n",
    "            elif i is 1:\n",
    "                w_h_2 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n",
    "                b_2 = build_shared_zeros(self.hidden_layer[i])\n",
    "                self.params.append(w_h_2)\n",
    "                self.params.append(b_2)\n",
    "            elif i is 2:\n",
    "                w_h_3 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n",
    "                b_3 = build_shared_zeros(self.hidden_layer[i])\n",
    "                self.params.append(w_h_3)\n",
    "                self.params.append(b_3)\n",
    "            elif i is 3:\n",
    "                w_h_4 = init_weights((self.hidden_layer[i-1], self.hidden_layer[i]))\n",
    "                b_4 = build_shared_zeros(self.hidden_layer[i])\n",
    "                self.params.append(w_h_4)\n",
    "                self.params.append(b_4)\n",
    "                \n",
    "            if (i+1) is len(hidden_layer):\n",
    "                w_o = init_weights((self.hidden_layer[i], self.output_num_units))\n",
    "                b_o = build_shared_zeros(self.output_num_units)\n",
    "                self.params.append(w_o)\n",
    "                self.params.append(b_o)\n",
    "                break\n",
    "        \n",
    "        self.update_learning_rate= theano.shared(floatX( up_learning_rate['start'] ))\n",
    "        \n",
    "        self.lr = np.linspace(up_learning_rate['start'], up_learning_rate['stop'], self.max_epochs)\n",
    "        \n",
    "        self.update_momentum= theano.shared(floatX( up_momentum['start'] ))\n",
    "        self.mm = np.linspace(up_momentum['start'], up_momentum['stop'], up_momentum['epoch'])\n",
    "    \n",
    "        X = T.dmatrix()\n",
    "        Y = T.dmatrix()\n",
    "        \n",
    "        py_x_drop = model(X, self.params, \n",
    "                     self.drop_input, self.drop_hidden, self.activation, True)\n",
    "    \n",
    "        py_x = model(X, self.params, \n",
    "                     self.drop_input, self.drop_hidden, self.activation, False)\n",
    "\n",
    "        y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "        cost = multinominal_cross_entropy(py_x_drop, Y) \n",
    "        \n",
    "        updates = momentum(cost, self.params, self.update_momentum, self.update_learning_rate, self.max_norm)\n",
    "\n",
    "        # Compile expressions to functions\n",
    "        self.train = theano.function(inputs=[X, Y], outputs=[cost], \n",
    "                                updates=updates, allow_input_downcast=True, name = \"train\")\n",
    "        self.predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True, name = \"predict\")\n",
    "\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        X_train, X_test, y_train, y_test = cv.train_test_split(x, y, test_size= self.eval_size)\n",
    "        yy = np.array(map((lambda x: np.argmax(x)), y_test))\n",
    "            \n",
    "        print \" \"\n",
    "        print \"start training!!!!\"\n",
    "        print \" \"\n",
    "            \n",
    "        epochs = 0\n",
    "        for i in range(self.max_epochs):\n",
    "            epochs +=1\n",
    "            t0 = time()\n",
    "            \n",
    "            # read mini batch data\n",
    "            for start, end in zip(range(0, len(X_train), self.batch), range(self.batch, len(X_train), self.batch)):\n",
    "                err = self.train(X_train[start:end], y_train[start:end])\n",
    "            \n",
    "            score = accuracy_score(yy, self.predict(X_test))\n",
    "            self.train_history_.append({\"epoch\":epochs, \"err\": err, \"score\":score})\n",
    "            \n",
    "            print 'epoch {0} : err = {1}, score = {2}, time ={3} s'.format(epochs, err, score, time() - t0)\n",
    "            \n",
    "            # deal with gradient exploit or vanish problem\n",
    "            if np.isnan(err):\n",
    "                for qq in range (len(self.params)):\n",
    "                    self.params[qq].set_value( self.best_params[qq] )\n",
    "                break\n",
    "            \n",
    "            '''\n",
    "            Early Stopping\n",
    "            '''\n",
    "            current_score = self.train_history_[-1]['score']\n",
    "            current_epoch = self.train_history_[-1]['epoch']\n",
    "            if current_score > self.best_valid:\n",
    "                self.best_valid = current_score\n",
    "                self.best_valid_epoch = current_epoch\n",
    "                self.best_params = [w.get_value() for w in self.params]\n",
    "            elif self.best_valid_epoch + self.patience <= current_epoch:\n",
    "                print \"\"\n",
    "                print \"Early stopping.\"\n",
    "                print self.best_valid_epoch,self.best_valid\n",
    "                print \"Best valid score {:.6f} at epoch {}.\".format(self.best_valid, self.best_valid_epoch)\n",
    "                \n",
    "                for qq in range (len(self.params)):\n",
    "                    self.params[qq].set_value( self.best_params[qq] )\n",
    "                break\n",
    "\n",
    "    def prediction(self, x):\n",
    "        return self.predict(x)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "net = DNN(\n",
    "        input_shape=(128,784), # 128 is mini batch size, 784 is data dimension\n",
    "        hidden_layer=[1500,1500,1500,1500 ], # maximum size to 6 layer only\n",
    "        drop_input=0.2, # usually is 0.2\n",
    "        drop_hidden=[0.5,0.5,0.5,0.5], # maximum size to 6 layer only, usually use 0.5 \n",
    "        activation = \"relu\", # relu, sigmoid\n",
    "        batch=128, \n",
    "        max_epochs=300, \n",
    "        eval_size=0.1, \n",
    "        output_num_units=48, \n",
    "        up_learning_rate = {'start':0.1, 'stop':0.0001}, # make it dynamic increase\n",
    "        up_momentum = {'start':0.5, 'stop':0.9, 'epoch':20}, # use small momentum first, then change to large momentum\n",
    "        patience=20, # early stopping\n",
    "        max_norm = None # need to tune, maybe start from 3 or 4\n",
    "    )\n",
    "\n",
    "net.fit( D[0], D[1] ) \n",
    "net.predict( your_testing_data )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And you can use cPickle to save / load model\n",
    "\n",
    "```python\n",
    "import cPickle as pickle\n",
    "with open('file_name.pickle', 'wb') as f:\n",
    "    pickle.dump(net, f, -1)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "- https://github.com/chanhou/MLDS_2015/blob/master/hw1/Theano_simple_version_7.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Application\n",
    "\n",
    "*** 重大聲明， 有些內容我也還沒有弄明白其中的原理 ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- CNN in image recognition\n",
    "- RNN in sequence learning\n",
    "- Deep Q learning in reinforcement learning\n",
    "- 不負責任的 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CNN in Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# My previous experience in NTU ML class\n",
    "\n",
    "https://github.com/chanhou/NTUML_Final\n",
    "\n",
    "<a href=https://github.com/chanhou/NTUML_Final/blob/master/final_project_preprocessing_attemp.ipynb>Data preprocess</a>\n",
    "\n",
    "<a href=https://github.com/chanhou/NTUML_Final/blob/master/Lasagne%20in%20the%20rock%201st%20try%20amazing%200.83.ipynb>  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Karpathy 大大的 demo \n",
    "\n",
    "個人網站： http://cs.stanford.edu/people/karpathy/\n",
    "\n",
    "http://cs.stanford.edu/people/karpathy/convnetjs/index.html\n",
    "\n",
    "http://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://cs.stanford.edu/people/karpathy/ width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://cs.stanford.edu/people/karpathy/ width=700 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://cs.stanford.edu/people/karpathy/convnetjs/index.html width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://cs.stanford.edu/people/karpathy/convnetjs/index.html width=700 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualizing tools for CNN\n",
    "\n",
    "https://github.com/bruckner/deepViz\n",
    "<figure><img src=\"https://raw.githubusercontent.com/bruckner/deepViz/master/main_screenshot.png\" alt=\"image\" ></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CNN + LSTM for image captioning\n",
    "\n",
    "- Use CNN to extract vector representation of the images\n",
    "- And using RNN + LSTM \n",
    "\n",
    "<figure><img src=\"deep_image/karpathy.png\" alt=\"image\" ></figure>\n",
    "\n",
    "Source: <a href=http://cs.stanford.edu/people/karpathy/cvpr2015.pdf> Deep Visual-Semantic Alignments for Generating Image Descriptions, Andrej Karpathy, Li Fei-Fei</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/ width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/ width=700 height=350></iframe>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DeepMind\n",
    "\n",
    "- Original DeepMind code: https://sites.google.com/a/deepmind.com/dqn/\n",
    "\n",
    "- Ilya Kuzovkin's fork with visualization:\n",
    "https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\n",
    "\n",
    "- This patch fixes the visualization when reloading a pre-trained network. The window will appear after the first evaluation batch is done (typically a few minutes):\n",
    "http://cg.tuwien.ac.at/~zsolnai/wp/wp-content/uploads/2015/03/train_agent.patch\n",
    "\n",
    "- This configuration file will run Ilya Kuzovkin's version with less than 1GB of VRAM:\n",
    "http://cg.tuwien.ac.at/~zsolnai/wp/wp-content/uploads/2015/03/run_gpu\n",
    "\n",
    "- The original Nature paper is available here:\n",
    "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Roughly:\n",
    "\n",
    "- Raw pixel input frm Atari games\n",
    "- keyboard keys as action space\n",
    "- Score and game over as reward \n",
    "- CNN trained with a Q-Learning variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/V1eYniJ0Rnk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fcba872ab10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('V1eYniJ0Rnk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning to execute\n",
    "\n",
    "- train a Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) units on short snippets of python code. The Network is trained to predict the output of the generated programs.\n",
    "```\n",
    "Input:\n",
    "    i=8827\n",
    "    c=(i-5347)\n",
    "    print( (c+8704) if 2641<8500 else 5308)\n",
    "Target: 12184.\n",
    "```\n",
    "Reference:\n",
    "\n",
    "http://arxiv.org/pdf/1410.4615v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Memory Network\n",
    "- Answering Queries (FB AI research)\n",
    "\n",
    "\n",
    "- Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk.\n",
    "- Joe travelled to the office. Joe left the milk. Joe went to the bathroom.\n",
    "- Where is the milk now? ***A: office***\n",
    "- Where is Joe? ***A: bathroom***\n",
    "- Where was Joe before the office? ***A: kitchen***\n",
    "\n",
    "Reference:\n",
    "    \n",
    "    Memory Networks,  \n",
    "http://arxiv.org/pdf/1410.3916v9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Walk\n",
    "\n",
    "Online Learning of Social Representations\n",
    "\n",
    "http://www.perozzi.net/projects/deepwalk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Plus\n",
    "\n",
    "https://plus.google.com/communities/112866381580457264725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References: Deep learning\n",
    "\n",
    "1. <a href=http://www.slideshare.net/yandex/yann-le-cun> Yann Le Cun:  Learning Hierarchies Of Invariant Features </a> \n",
    "- <a href=http://neuralnetworksanddeeplearning.com/index.html> Online book: Neural Networks and Deep Learning </a>\n",
    "- <a href=http://ufldl.stanford.edu/tutorial/> Online tutorial: Stanford Deep Learning </a>\n",
    "- <a href=https://github.com/ChristosChristofidis/awesome-deep-learning> Awesome Deep learning Github </a>\n",
    "- <a href=http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/> 100 Best Github deep learning </a>\n",
    "- <a href=http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/> Using convolutional neural nets to detect facial keypoints tutorial </a>\n",
    "- <a href=http://www.iro.umontreal.ca/~bengioy/yoshua_en/research.html> Yoshua Bengio </a>\n",
    "- <a href=https://www.coursera.org/course/neuralnets> Hinton Coursera website </a>\n",
    "- <a href=https://speakerdeck.com/ogrisel/the-state-of-deep-learning-in-1> The State of Deep Learning in 2014 </a>\n",
    "- 四部曲<a href=http://www.36dsj.com/archives/20804> 神经网络简史 </a>\n",
    "- 四部曲<a href=http://www.36dsj.com/archives/20186> 深度学习系列：Mariana DNN多GPU数据并行框架 微信语音是怎么来的？（二） </a>\n",
    "- 四部曲<a href=http://www.36dsj.com/archives/20200> 深度学习系列：Mariana CNN并行框架与图像识别（三） </a>\n",
    "- 四部曲<a href=http://www.36dsj.com/archives/19285> 深度学习系列：解密最接近人脑的智能学习机器——深度学习及并行化实现（四） </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References: Other\n",
    "\n",
    "1. <a href=https://www.coursera.org/course/ntumlone>NTU ML Hsuan Tien-Lin class</a> \n",
    "- <a href=http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/>Differences between L1 and L2 as Loss Function and Regularization</a> \n",
    "- <a href=http://deeplearning.net/software/theano/tutorial/index.html#tutorial>Theano tutorial</a> "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
